# crawler/notice_spider.py
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import os
from urllib.parse import urljoin

# å¤§å­¦å®˜ç½‘åŸºç¡€åœ°å€
BASE_URL = "https://XXX/"
NOTICE_URL = "https://XXX/newtzgg-list.jsp?urltype=tree.TreeTempUrl&wbtreeid=1187"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def parse_chinese_date(chinese_date: str) -> str:
    """
    å°†â€œåæœˆ22â€è½¬æ¢ä¸º â€œ2025-10-22â€
    å½“å‰å¹´ä»½ + ä¸­æ–‡æœˆä»½æ˜ å°„ + æ—¥æœŸ
    """
    year = datetime.now().year
    month_map = {
        'ä¸€æœˆ': '01', 'äºŒæœˆ': '02', 'ä¸‰æœˆ': '03', 'å››æœˆ': '04',
        'äº”æœˆ': '05', 'å…­æœˆ': '06', 'ä¸ƒæœˆ': '07', 'å…«æœˆ': '08',
        'ä¹æœˆ': '09', 'åæœˆ': '10', 'åä¸€æœˆ': '11', 'åäºŒæœˆ': '12'
    }
    for k, v in month_map.items():
        if chinese_date.startswith(k):
            day = chinese_date.replace(k, '').strip()
            return f"{year}-{v}-{int(day):02d}"
    return "æœªçŸ¥"

def crawl_notices():
    print("ğŸš€ å¼€å§‹çˆ¬å–å¤§å­¦é€šçŸ¥å…¬å‘Š...")
    response = requests.get(NOTICE_URL, headers=HEADERS)
    response.raise_for_status()
    response.encoding = 'utf-8'  # å¼ºåˆ¶ UTF-8 é˜²æ­¢ä¹±ç 
    print(response.status_code, response.url)
    # print(response.text)

    soup = BeautifulSoup(response.text, 'html.parser')
    notice_items = soup.select('div.nwu-not ul li')
    #notice_items = soup.select('div.news-item')  # æ‰€æœ‰é€šçŸ¥é¡¹

    notices = []

    for item in notice_items:
        date_span = item.select_one('span.date')
        title_link = item.select_one('a')
        dept_span = item.select_one('span.department')

        if not title_link:
            continue

        # æå–ä¿¡æ¯
        raw_date = date_span.get_text(strip=True) if date_span else "æœªçŸ¥"
        title = title_link.get_text(strip=True)
        url = urljoin(BASE_URL, title_link['href'])  # æ‹¼æ¥å®Œæ•´ URL
        department = dept_span.get_text(strip=True) if dept_span else "æœªçŸ¥"

        # è½¬æ¢ä¸­æ–‡æ—¥æœŸä¸ºæ ‡å‡†æ ¼å¼
        publish_date = parse_chinese_date(raw_date)

        # åˆå§‹åŒ– content å­—æ®µï¼ˆåç»­å†æŠ“ï¼‰
        notices.append({
            "title": title,
            "url": url,
            "publish_date": publish_date,
            "raw_date": raw_date,  # ä¿ç•™åŸå§‹æ—¥æœŸ
            "department": department,
            "crawl_time": datetime.now().isoformat(),
            "content": ""
        })

    # ä¿å­˜åŸå§‹æ•°æ®
    os.makedirs("data/raw", exist_ok=True)
    filename = f"data/raw/notices_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(notices, f, ensure_ascii=False, indent=2)

    print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…± {len(notices)} æ¡é€šçŸ¥ï¼Œä¿å­˜åˆ° {filename}")
    return notices


def crawl_all_notices(max_pages=5):  # å…ˆçˆ¬5é¡µåšæµ‹è¯•
    all_notices = []
    for page in range(1, max_pages + 1):
        print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
        page_url = f"https://www.nwu.edu.cn/newtzgg-list.jsp?urltype=tree.TreeTempUrl&wbtreeid=1187&PAGENUM={page}"
        response = requests.get(page_url, headers=HEADERS)
        response.raise_for_status()
        response.encoding = 'utf-8'

        soup = BeautifulSoup(response.text, 'html.parser')
        notice_items = soup.select('div.nwu-not ul li')

        for item in notice_items:
            month = item.select_one('time .month').get_text(strip=True)
            day = item.select_one('time .day').get_text(strip=True)
            raw_date = f"{month}{day}"

            title_link = item.select_one('a')
            title = title_link.get('title', '').strip()
            href = title_link.get('href', '').strip()
            url = urljoin(BASE_URL, href)

            dept_span = item.select_one('.related-site a')
            department = dept_span.get_text(strip=True) if dept_span else "æœªçŸ¥"

            publish_date = parse_chinese_date(raw_date)

            all_notices.append({
                "title": title,
                "url": url,
                "publish_date": publish_date,
                "raw_date": raw_date,
                "department": department,
                "crawl_time": datetime.now().isoformat(),
                "content": ""
            })

    # ä¿å­˜
    os.makedirs("./data/raw", exist_ok=True)
    filename = f"./data/raw/notices_all_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(all_notices, f, ensure_ascii=False, indent=2)

    print(f"æ‰€æœ‰é€šçŸ¥çˆ¬å–å®Œæˆï¼Œå…± {len(all_notices)} æ¡ï¼Œä¿å­˜åˆ° {filename}")
if __name__ == '__main__':
    crawl_all_notices(max_pages=3)  # å…ˆçˆ¬3é¡µè¯•è¯•
